{
  "pages": [
    {
      "title": "Resources for learning R",
      "tags": "learning,R,resources,tutorials",
      "url": "http://people.fas.harvard.edu/~izahn/posts/resources-for-learning-r/",
      "text": "A list of resources for learning R in preparation for CS109 this Spring.\n\n\n\nA wealth of R resources are available, and I'm sure I've missed some really good ones. If you have a favorite tutorial or resource that is not listed here, please email me or submit a bug report or pull request to http://github.com/izahn/blog.\n\n\n\nTutorials\n\n\nInteractive\n\n\nThere are some great efforts to provide interactive self-paced R tutorials in your browser or in R itself.\n\n\n\n https://www.datacamp.com/ Interactive R tutorials with feedback, right in your web browser!\n\n http://swirlstats.com/students.html Interactive R tutorials with feedback in R.\n\n http://dss.iq.harvard.edu/workshop-materials#widget-1 Interactive R tutorials in your web browser. Includes a ggplot tutorial.\n\n\n\n\n\n\n\nStatic\n\n\nMany R tutorials have been collected at https://www.r-project.org/other-docs.html. The list of contributed documentation at https://cran.r-project.org/other-docs.html is a great place to start.\n\n\n\nThere are several excellent tutorials not listed on r-project.org. Some of these are listed below.\n\n\n\n http://www.statmethods.net/ \"Quick-R\" aims to get you up and running in R quickly.\n\n http://personality-project.org/r/r.guide.html Notes on \"Using R for psychological research\". \n\n http://r4ds.had.co.nz/ \"R for Data Science\" by R luminary Hadley Wickham. Includes a ggplot tutorial.\n\n http://rmarkdown.rstudio.com/lesson-1.html A comprehensive RMarkdown tutorial.\n\n\n\n\n\n\n\nReference cards\n\n\nRStudio maintains a collection of high-quality cheat sheets at https://www.rstudio.com/resources/cheatsheets/ (these are also accessible from the Help -> cheat-sheets menu in the RStudio IDE). Additional resources are listed below.\n\n\n\n http://mathesaurus.sourceforge.net/r-numpy.html A numpy cheat sheet for R users, but it works just as well the other way around.\n\n http://www.math.umaine.edu/~hiebeler/comp/matlabR.pdf An R cheat sheet for MATLAB users.\n\n http://mathesaurus.sourceforge.net/matlab-python-xref.pdf Another R cheat sheet for MATLAB or Python users.\n\n\n\n\n\n\nR package discovery\n\n\nThe Comprehensive R Archive Network (CRAN) is the main R package repository. The web interface is not very sophisticated, so I recommend using the resources listed below instead.\n\n\n\n https://cran.r-project.org/web/views/ R Task Views are curated lists of R packages and functions organized by topic.\n\n http://r-pkg.org METACRAN is a friendly, search-able web interface to CRAN.\n\n http://rdocumentation.org A search-able interactive interface to R and R package documentation.\n\n\n\n\n\n\nBlogs, forums and mailing lists\n\n\nR related blogs are aggregated at http://r-bloggers.com. \n\n\n\nhttp://stackoverflow.com is by far the most popular help forum for R. Use the [r] tag or navigate directly to http://stackoverflow.com/questions/tagged/r.\n\n\n\nAlthough the R mailing lists have been losing traffic to stackoverflow there are still plenty of people (including me) responding to questions. You can subscribe to the main R-help mailing list at https://stat.ethz.ch/mailman/listinfo/r-help."
    },
    {
      "title": "Coming to terms with the pace of change in R",
      "tags": "R",
      "url": "http://people.fas.harvard.edu/~izahn/posts/coming-to-terms-with-the-pace-of-change-in-r/",
      "text": "Is it you or have I become old and cranky?\n\n\nI've been using R and mostly enjoying it since 2006. Lately I've been having some misgivings about the direction R as a community is headed. Some of these misgivings no doubt stem from reluctance to learn new ways of doing things after investing so much time mastering the old ways, but underneath my old-man crankiness I believe there are real and important challenges facing the R community. R has grown considerably since I started using it a decade ago, and this has mostly been a good thing as new packages implement new and better functionality. Recently, popular contributed packages have been replacing core R functionality with new approaches, leading to a fragmentation of the user base and increasing cognitive load by requiring analysts to choose a package (or set of packages) before they even write the first line of code.\n\n\n\n\n\n\n\nR is \"a large, coherent, integrated collection of intermediate tools for data analysis\"\n\n\nThe quote is taken from the official R manual An Introduction to R. The \"coherent\" and \"integrated\" claims have always been questionable in some areas, but this situation is getting far worse as contributed packages re-invent basic R features and replace classic R idioms with new ones. Let me show you what I'm talking about.\n\n\n\n\nIs there a coherent integrated \"group by\" in R?\n\n\nSuppose I want to aggregate a data set by calculating group sizes, means, and standard deviations. There are many ways to do it in base R, and none are quite satisfactory.\n\n\n\nFirst up is the tapply function. It works well for simple aggregation, but won't help us here because it can only handle a single x variable, and it produces confusing return values if FUN returns a vector of length greater than one. That is to say, it works fine for calculating a single summary statistic by group\n\ntapply(mtcars$wt, mtcars[c(\"am\", \"cyl\")],\n       FUN = mean)\n\n\n\n   cyl\nam        4       6        8\n  0 2.93500 3.38875 4.104083\n  1 2.04225 2.75500 3.370000\n\n\n\nbut anything more complicated quickly becomes a mess\n\ntapply(mtcars$wt, mtcars[c(\"am\", \"cyl\")],\n\tFUN = function(x) c(n = length(x), mean = mean(x), sd = sd(x)))\n\n\n\n   cyl\nam  4         6         8        \n  0 Numeric,3 Numeric,3 Numeric,3\n  1 Numeric,3 Numeric,3 Numeric,3\n\n\n\nNext we might consider the ave function, but a quick look at the documentation suggests it won't be any better than tapply. OK, let's try something else. Maybe aggregate will help us.\n\naggregate(cbind(mpg, hp, wt) ~ am + cyl,\n\t  data = mtcars,\n\t  FUN = function(x) c(n = length(x), mean = mean(x), sd = sd(x)))\n\n\n\n  am cyl      mpg.n   mpg.mean     mpg.sd      hp.n   hp.mean     hp.sd       wt.n    wt.mean      wt.sd\n1  0   4  3.0000000 22.9000000  1.4525839   3.00000  84.66667  19.65536  3.0000000  2.9350000  0.4075230\n2  1   4  8.0000000 28.0750000  4.4838599   8.00000  81.87500  22.65542  8.0000000  2.0422500  0.4093485\n3  0   6  4.0000000 19.1250000  1.6317169   4.00000 115.25000   9.17878  4.0000000  3.3887500  0.1162164\n4  1   6  3.0000000 20.5666667  0.7505553   3.00000 131.66667  37.52777  3.0000000  2.7550000  0.1281601\n5  0   8 12.0000000 15.0500000  2.7743959  12.00000 194.16667  33.35984 12.0000000  4.1040833  0.7683069\n6  1   8  2.0000000 15.4000000  0.5656854   2.00000 299.50000  50.20458  2.0000000  3.3700000  0.2828427\n\n\n\naggregate appears to be closer to what we want than tapply, but it still either requires post-processing to remove the redundant group size column, or multiple calls to produce the desired result, e.g.,\n\nmerge(aggregate(cbind(n = mpg) ~ am +cyl,\n\t\tdata = mtcars,\n\t\tFUN = length),\n      aggregate(cbind(mpg, hp, wt) ~ am + cyl,\n\t\tdata = mtcars,\n\t\tFUN = function(x) c(mean = mean(x), sd = sd(x))))\n\n\n\n  am cyl  n   mpg.mean     mpg.sd   hp.mean     hp.sd   wt.mean     wt.sd\n1  0   4  3 22.9000000  1.4525839  84.66667  19.65536 2.9350000 0.4075230\n2  0   6  4 19.1250000  1.6317169 115.25000   9.17878 3.3887500 0.1162164\n3  0   8 12 15.0500000  2.7743959 194.16667  33.35984 4.1040833 0.7683069\n4  1   4  8 28.0750000  4.4838599  81.87500  22.65542 2.0422500 0.4093485\n5  1   6  3 20.5666667  0.7505553 131.66667  37.52777 2.7550000 0.1281601\n6  1   8  2 15.4000000  0.5656854 299.50000  50.20458 3.3700000 0.2828427\n\n\n\nOK, tapply and ave were busts, aggregate is close but not quite what we want. How about by?\n\n\nby(mtcars,\n   mtcars[c(\"am\", \"cyl\")],\n   FUN = function(x)\n   {\n     with(x,\n\t  c(n = nrow(x),\n\t    mpg_mean = mean(mpg),\n\t    mpg_sd = sd(mpg),\n\t    hp_mean = mean(hp),\n\t    hp_sd = sd(hp),\n\t    wt_mean = mean(wt),\n\t    wt_sd = sd(wt)))\n   },\n   simplify = FALSE)\n\n\n\n am: 0\ncyl: 4\n        n  mpg_mean    mpg_sd   hp_mean     hp_sd   wt_mean     wt_sd \n 3.000000 22.900000  1.452584 84.666667 19.655364  2.935000  0.407523 \n------------------------------------------------------------------------------------- \nam: 1\ncyl: 4\n         n   mpg_mean     mpg_sd    hp_mean      hp_sd    wt_mean      wt_sd \n 8.0000000 28.0750000  4.4838599 81.8750000 22.6554156  2.0422500  0.4093485 \n------------------------------------------------------------------------------------- \nam: 0\ncyl: 6\n          n    mpg_mean      mpg_sd     hp_mean       hp_sd     wt_mean       wt_sd \n  4.0000000  19.1250000   1.6317169 115.2500000   9.1787799   3.3887500   0.1162164 \n------------------------------------------------------------------------------------- \nam: 1\ncyl: 6\n          n    mpg_mean      mpg_sd     hp_mean       hp_sd     wt_mean       wt_sd \n  3.0000000  20.5666667   0.7505553 131.6666667  37.5277675   2.7550000   0.1281601 \n------------------------------------------------------------------------------------- \nam: 0\ncyl: 8\n          n    mpg_mean      mpg_sd     hp_mean       hp_sd     wt_mean       wt_sd \n 12.0000000  15.0500000   2.7743959 194.1666667  33.3598379   4.1040833   0.7683069 \n------------------------------------------------------------------------------------- \nam: 1\ncyl: 8\n          n    mpg_mean      mpg_sd     hp_mean       hp_sd     wt_mean       wt_sd \n  2.0000000  15.4000000   0.5656854 299.5000000  50.2045815   3.3700000   0.2828427\n\n\n\nWell, maybe that's better. It's not really any less verbose than the aggregate-and-merge strategy, and the result isn't very friendly. Maybe we should just roll our own.\n\ndo.call(rbind,\n\tlapply(split(mtcars, mtcars[c(\"am\", \"cyl\")]),\n\t       function(x) {\n\t\t with(x, \n\t\t      data.frame(am = unique(am),\n\t\t\t\t cyl = unique(cyl),\n\t\t\t\t n = nrow(x),\n\t\t\t\t mpg_mean = mean(mpg),\n\t\t\t\t mpg_sd = sd(mpg),\n\t\t\t\t hp_mean = mean(hp),\n\t\t\t\t hp_sd = sd(hp),\n\t\t\t\t wt_mean = mean(wt),\n\t\t\t\t wt_sd = sd(wt)))\n\t       }))\n\n\n\n    am cyl  n mpg_mean    mpg_sd   hp_mean    hp_sd  wt_mean     wt_sd\n0.4  0   4  3 22.90000 1.4525839  84.66667 19.65536 2.935000 0.4075230\n1.4  1   4  8 28.07500 4.4838599  81.87500 22.65542 2.042250 0.4093485\n0.6  0   6  4 19.12500 1.6317169 115.25000  9.17878 3.388750 0.1162164\n1.6  1   6  3 20.56667 0.7505553 131.66667 37.52777 2.755000 0.1281601\n0.8  0   8 12 15.05000 2.7743959 194.16667 33.35984 4.104083 0.7683069\n1.8  1   8  2 15.40000 0.5656854 299.50000 50.20458 3.370000 0.2828427\n\n\n\nBy now we've tried four different approaches, but nothing seems to make the calculation particularly natural or convenient. Is this really a \"coherent and integrated\" collection of functions? It feels more like a haphazard collection of overlapping functions that can be abused in different ways. So here are some questions. \n\n\nGiven that aggregate appears to be more flexible than tapply and ave, do we really need the later two?\n\nCan aggregate be generalized so that we can apply functions to data.frames instead of to the columns of those data.frames?\n\n\n\n\n\n\n\nCan we do better?\n\n\nOf course we can do better. Many an R programmer has gazed out over the rubble of tapply, ave, by and aggregate and mused \"surely I can bring order and harmony to this jumble. Follow me and we will create a 'group by' operation to end all SQL jealousy in the kingdom of R.\" And what comes of this musing? Let us look with wonder upon the bubbling exuberant creativity of the R community.\n\n\n\n\ndoBy::describeBy\n\n\nVery similar to aggregate, same limitations.\n\ndoBy::summaryBy(mpg + hp + wt ~ am + cyl,\n\t\tdata = mtcars,\n\t\tFUN = function(x) c(n = length(x), mean = mean(x), sd = sd(x)))\n\n\n\n  am cyl mpg.n mpg.mean    mpg.sd hp.n   hp.mean    hp.sd wt.n  wt.mean     wt.sd\n1  0   4     3 22.90000 1.4525839    3  84.66667 19.65536    3 2.935000 0.4075230\n2  0   6     4 19.12500 1.6317169    4 115.25000  9.17878    4 3.388750 0.1162164\n3  0   8    12 15.05000 2.7743959   12 194.16667 33.35984   12 4.104083 0.7683069\n4  1   4     8 28.07500 4.4838599    8  81.87500 22.65542    8 2.042250 0.4093485\n5  1   6     3 20.56667 0.7505553    3 131.66667 37.52777    3 2.755000 0.1281601\n6  1   8     2 15.40000 0.5656854    2 299.50000 50.20458    2 3.370000 0.2828427\n\n\n\n\n\nHmisc::summary.formula\n\n\nSimilar to aggregate, large number of confusing options. This one automatically computes N for each group, so it actually works for our example.\n\nHmisc::summary.formula(cbind(mpg, hp, wt) ~ am + cyl,\n\t\t       data = mtcars,\n\t\t       fun = function(x) {\n\t\t\t apply(x,\n\t\t\t       2,\n\t\t\t       FUN = function(y) {\n\t\t\t\t c(mean = mean(y), sd = sd(y))\n\t\t\t       })\n\t\t       })\n\n\n\n cbind(mpg, hp, wt)    N=32\n\n+-------+---+--+--------+--------+---------+--------+--------+---------+\n|       |   |N |mpg mean|mpg sd  |hp mean  |hp sd   |wt mean |wt sd    |\n+-------+---+--+--------+--------+---------+--------+--------+---------+\n|am     |No |19|17.14737|3.833966|160.26316|53.90820|3.768895|0.7774001|\n|       |Yes|13|24.39231|6.166504|126.84615|84.06232|2.411000|0.6169816|\n+-------+---+--+--------+--------+---------+--------+--------+---------+\n|cyl    |4  |11|26.66364|4.509828| 82.63636|20.93453|2.285727|0.5695637|\n|       |6  | 7|19.74286|1.453567|122.28571|24.26049|3.117143|0.3563455|\n|       |8  |14|15.10000|2.560048|209.21429|50.97689|3.999214|0.7594047|\n+-------+---+--+--------+--------+---------+--------+--------+---------+\n|Overall|   |32|20.09062|6.026948|146.68750|68.56287|3.217250|0.9784574|\n+-------+---+--+--------+--------+---------+--------+--------+---------+\n\n\n\n\n\ndplyr::summarize\n\n\nThis one is very popular, and for good reason. It works well.\n\ndplyr::summarize(dplyr::group_by(mtcars, am, cyl),\n\t\t n = length(mpg),\n\t\t mean_mpg = mean(mpg),\n\t\t sd_mpg = sd(mpg),\n\t\t mean_hp = mean(hp),\n\t\t sd_hp = sd(hp),\n\t\t mean_wt = mean(wt),\n\t\t sd_hp = sd(hp))\n\n\n\n Source: local data frame [6 x 8]\nGroups: am [?]\n\n     am   cyl     n mean_mpg    sd_mpg   mean_hp    sd_hp  mean_wt\n  <dbl> <dbl> <int>    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>\n1     0     4     3 22.90000 1.4525839  84.66667 19.65536 2.935000\n2     0     6     4 19.12500 1.6317169 115.25000  9.17878 3.388750\n3     0     8    12 15.05000 2.7743959 194.16667 33.35984 4.104083\n4     1     4     8 28.07500 4.4838599  81.87500 22.65542 2.042250\n5     1     6     3 20.56667 0.7505553 131.66667 37.52777 2.755000\n6     1     8     2 15.40000 0.5656854 299.50000 50.20458 3.370000\n\n\n\n\n\ndplyr::do\n\n\nIf you have a large number of columns to summarize you might not want to type them all out. In that case you can use do.\n\ndo(group_by(mtcars, am, cyl),\n   as.data.frame(c(list(n = ncol(.)),\n\t\t   as.list(sapply(.[c(\"mpg\", \"wt\", \"hp\")],\n\t\t\t\t  function(x) c(mean = mean(x)))),\n\t\t   as.list(sapply(.[c(\"mpg\", \"wt\", \"hp\")],\n\t\t\t\t  function(x) c(sd = mean(x)))))))\n\n\n\n Source: local data frame [6 x 9]\nGroups: am, cyl [6]\n\n     am   cyl     n mpg.mean  wt.mean   hp.mean   mpg.sd    wt.sd     hp.sd\n  <dbl> <dbl> <int>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1     0     4    11 22.90000 2.935000  84.66667 22.90000 2.935000  84.66667\n2     0     6    11 19.12500 3.388750 115.25000 19.12500 3.388750 115.25000\n3     0     8    11 15.05000 4.104083 194.16667 15.05000 4.104083 194.16667\n4     1     4    11 28.07500 2.042250  81.87500 28.07500 2.042250  81.87500\n5     1     6    11 20.56667 2.755000 131.66667 20.56667 2.755000 131.66667\n6     1     8    11 15.40000 3.370000 299.50000 15.40000 3.370000 299.50000\n\n\n\n\n\ntables::tabular\n\n\nThis one focuses on creating LaTeX and HTML tables. It creates its own SAS-inspired mini-language that is IMO very confusing, though possibly worth it if you frequently need to create complex publication ready tables.\n\n#library(tables)\ntables::tabular((Factor(am))*(Factor(cyl)) ~ (n = 1) + (mpg + wt + hp)*(mean + sd), data = mtcars)\n\n\n\n                                                 \n          mpg          wt           hp           \nam cyl n  mean  sd     mean  sd     mean   sd    \n0  4    3 22.90 1.4526 2.935 0.4075  84.67 19.655\n   6    4 19.12 1.6317 3.389 0.1162 115.25  9.179\n   8   12 15.05 2.7744 4.104 0.7683 194.17 33.360\n1  4    8 28.07 4.4839 2.042 0.4093  81.88 22.655\n   6    3 20.57 0.7506 2.755 0.1282 131.67 37.528\n   8    2 15.40 0.5657 3.370 0.2828 299.50 50.205\n\n\n\n\n\ndata.table::`[`\n\n\nThe data.table package implements an alternative to the venerable data.frame class in R and provides sophisticated manipulation via an indexing-like interface.\n\nas.data.table(mtcars)[,\n\t\t      list(n = .N,\n\t\t\t   mpg_mean = mean(mpg),\n\t\t\t   mpg_sd = sd(mpg),\n\t\t\t   wt_mean = mean(wt),\n\t\t\t   wt_sd = sd(wt),\n\t\t\t   hp_mean = mean(hp),\n\t\t\t   hp_sd = sd(hp)),\n\t\t      by = c(\"am\", \"cyl\")]\n\n\n\nAre we done yet? Well, I'm going to stop, but we could go on. There are at least 9 ways to skin this particular cat in R. \n\n\n\n\n\n\nHow do I _ in R?\n\n\nSo there are lots of ways to calculate statistics by some grouping variable(s) in R. Why can't you be happy that you have so many excellent choices?\n\n\n\nI can't be happy about it because it makes my life more difficult. First, I need to identify my options. Then I need to evaluate them, and learn the particulars of my chosen package. This all takes effort that I would rather spend on other things. Now, if this problem was limited to the domain of calculating statistics by group, I wouldn't be writing this post. But this issue is almost everywhere in R.\n\n\n\n\nHow do I read text data?\n\n\nI have a .csv file I want to read into R. Should I use\n\n\nread.csv\n\nreadr::readcsv\n\ndata.table::fread\n\nrio::import\n\nhypoparsr::parsefile\n\ncvsread::cvsread\n\n\n\nor something else?\n\n\n\n\n\nHow do I fit a linear regression model?\n\n\nI want to fit a simple linear regression model. Should I use\n\n\nlm\n\nrms::ols\n\nZelig::zlm\n\nglm2::glm2\n\n\n\nor something else?\n\n\n\n\n\nHow do I make a table from model coefficients?\n\n\nI've fit a model and would like to put the results in a nice table. Should I use\n\n\nxtable::xtable\n\nrockchalk::outreg\n\napsrtable::apsrtable\n\nhtmlTable::htmlTable\n\netable::tabular.ade\n\nknitr::ktable\n\ntexreg::texreg\n\nstargazer::stargazer\n\nascii::ascii\n\nestout::esttab\n\n\n\nor some other thing?\n\n\n\n\n\n\nTODO Summary [summarize the post thus far]\n\n\nThere is an overwhelming number of choices for doing just about anything in R.\n\n\n\n\n\n\nWhat is R really?\n\n\nOK, so there is some duplication among R functions and packages and people need to choose. There are both good and bad consequences of this, but the totality of the situation is that it is no longer clear what R is.\n\n\n\n\nR is not data.frames\n\n\nMost people who use R use it for statistical analysis and graphics. The basic data structure in most popular statistical package is a rectangular structure with variables in the columns and observations in the rows. In R this structure is called a data.frame and learning how to operate on and with data.frames is a basic skill that any R user must have.\n\n\n\nThe previous sentence may have been true at one time, but it no longer is. There are now at least three popular alternatives; data.frame, data.table and tibble. It is now possible to carry out sophisticated data manipulation and analysis in R without ever learning fundamental data.frame methods such as `[.data.frame`. All of these methods work differently.\n\nhead(mtcars)[, 1]\n\n\n\n[1] 21.0 21.0 22.8 21.4 18.7 18.1\n\n\nas.data.table(head(mtcars))[, 1]\n\n\n\n[1] 1\n\n\nas_tibble(head(mtcars))[, 1]\n\n\n\n# A tibble: 6 \u00d7 1\n    mpg\n  <dbl>\n1  21.0\n2  21.0\n3  22.8\n4  21.4\n5  18.7\n6  18.1\n\n\n\n\n\nR is not a language that uses parenthetical argument lists, like c\n\n\nOne of the first things I used to teach people about R is that function calls have the form functionName(arg1, arg2, ..., argn), and that even when all arguments are optional and you want to accept the defaults you need the () after the function name. This is no longer true. Many people now write strange looking R expressions like\n\nlibrary(magrittr)\nmtcars %>% head\n\n\n\nR has always had flexible syntax, but with developments like this you can write R that looks nothing like what us old-timers expect R code to be.\n\n\n\n\n\nR is not coherent\n\n\nThere is no zen-like \"one obvious way to do it\" it R. \n\n\n\n\n\nR is being built phoenix-like from its own ashes\n\n\nThere is good news and there is bad news. The good news is that new and more coherent and integrated zones are being carved out of the R landscape. For example, the tidyverse brings greater simplicity and constancy to many common operations in R. The bad news is that you can't escape the cold hard XKCD reality that producing a \"better\" way of doing things means there is one more way of doing that thing. \n\n\n\n\n\n\nAnd now, the thrilling conclusion\n\n\nNow finally we've reach the part of the blog where I tell you how everyone is doing it wrong and if you would just listen to me we could solve all our problems. The truth is I don't have great answers or solutions for these issues. The best I can do is offer some general thoughts.\n\n\n\n\nIt could be worse\n\n\nThis post probably sounds critical of R, but don't get me wrong, I'm a huge fan of R. Every time I venture into Python, Javascript, Scala, or even Julia it makes me appreciate R even more. R is easy and useful, and having too much choice is certainly better than having too little.\n\n\n\n\n\nWe can do better\n\n\nCollaborate more. Update documentation to recommend current best-of-breed packages."
    },
    {
      "title": "Extracting content from .pdf files",
      "tags": "OCR,pdf,R",
      "url": "http://people.fas.harvard.edu/~izahn/posts/extracting-content-from-pdf-files/",
      "text": "One of common question I get as a data science consultant involves extracting content from .pdf files. In the best-case scenario the content can be extracted to consistently formatted text files and parsed from there into a usable form. In the worst case the file will need to be run through an optical character recognition (OCR) program to extract the text.\n\n\n\nOverview of available tools\n\n\nFor years pdftotext from the poppler project was my go-to answer for the easy case. This is still a good option, especially on Mac (using homebrew) or Linux where installation is easy. Windows users can install poppler binaries from http://blog.alivate.com.au/poppler-windows/ (make sure to add the bin directory to your PATH). More recently I've been using the excellent pdftools packge in R to more easily extract and manipulate text stored in .pdf files.\n\n\n\nIn the more difficult case where the pdf contains images rather than text it is necessary to use optical character recognition (OCR) to recover the text. This can be achieved using point-and-click applications like freeOCR, Adobe Acrobat or ABBYY. ABBYY even has a convenient cloud OCR service that can be easily accessed from R using the abbyyR package. If you don't have a license for one of these expensive OCR solutions, or if you prefer something you easily can script from the command line, tesseract is a very good option.\n\n\n\n\n\n\n\nAn easy example\n\n\nIn the case where the pdf contains text, extracting it is usually not too difficult. As an example, consider the .pdf file at http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf. Wouldn't it be nice to extract the data in those tables so we can visualize it in different ways?1 We can, using the pdftotext utility provided by the poppler project.\n\n\ncurl -o nvsr65_05.pdf http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf\npdftotext nvsr65_05.pdf nvsr65_05.txt\nhead nvsr65_05.txt\n\n\n\nNational Vital\nStatistics Reports\nVolume 65, Number 5\n\nJune 30, 2016\n\nDeaths: Leading Causes for 2014\nby Melonie Heron, Ph.D., Division of Vital Statistics\n\nAbstract\n\n\n\nWe can achieve a similar result in R using the pdftools package.\n\n\nlibrary(pdftools)\nnvsr65_05 <- pdf_text(\"http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf\")\nhead(strsplit(nvsr65_05[[1]], \"\\n\")[[1]])\n\n\n\n[1] \"National Vital\"                                                                                                                          \n[2] \"Statistics Reports\"                                                                                                                      \n[3] \"Volume 65, Number 5                                                                                                        June 30, 2016\"\n[4] \"Deaths: Leading Causes for 2014\"                                                                                                         \n[5] \"by Melonie Heron, Ph.D., Division of Vital Statistics\"                                                                                   \n[6] \"Abstract                                                                 Introduction\"\n\n\n\nOnce the text has been liberated from the pdf we can parse it into a usable form and proceed from there. This is often tedious and delicate work, but with some care the data can usually be coerced into shape. For example, table G can be extracted using a few well crafted regular expressions.\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(magrittr)\nlibrary(dplyr)\ntable_data <- nvsr65_05[[14]] %>%\n  str_split(pattern = \"\\n\") %>%\n  unlist() %>%\n  str_subset(pattern = \"[\u2026].*(\\\\. ){5}\") %>%\n  str_c(collapse = \"\\n\") %>%\n  read_table(col_names = FALSE) %>%\n  mutate(X2 = str_replace_all(X2, \"(\\\\. )*\", \"\"),\n\t X5 = rep(c(\"Neonatal\", \"Postnatal\"), each = 10)) %>%\n  set_names(value = c(\"rank\", \"cause_of_death\", \"deaths\", \"percent\", \"group\"))\ntable_data\n\n\n\n# A tibble: 20 x 5\n    rank\n   <int>\n1      1\n2      2\n3      3\n4      4\n5      5\n6      6\n7      7\n8      8\n9      9\n10    10\n11     1\n12     2\n13     3\n14     4\n15     5\n16     6\n17     7\n18     8\n19     9\n20    10\n# ... with 4 more variables: cause_of_death <chr>, deaths <dbl>, percent <dbl>,\n#   group <chr>\n\n\n\nOnce the data has been liberated from the .pdf it can be used anyway we like\u2013for example, we can convert the table to a graph to make it easier to compare the prevelance of different causes.\n\nlibrary(ggplot2)\nggplot(mutate(table_data, cause_of_death = reorder(cause_of_death, deaths)),\n       aes(x = cause_of_death, y = percent)) +\n  geom_bar(aes(fill = deaths), stat=\"identity\") +\n  facet_wrap(~group) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nA more difficult example\n\n\nThe example above was relatively easy, because the pdf contained information stored as text. For many older pdfs (especialy old scanned documents) the information will instead be stored as images. This makes life much more difficult, but with a little work the data can be liberated. This example pdf file contains a code-book for old employment data sets. Lets see if this information can be extracted into a machine-readable form.\n\n\n\nAs mentioned in Overview of available tools there are several optinos to choose from. In this example I'm going to use tesseract because it is free and easily script-able. The tesseract program cannot process pdf files directly, so the first step is to convert each page of the pdf to an image. This can be done using the pdftocairo utility (part of the poppler project). The information I want is on pages 32 to 186, so I'll convert just those pages.\n\ncd ../files/example_files/blog/pdf_extraction\npdftocairo -png BLS_employment_costs_documentation.pdf -f 32 -l 186\n\n\n\nOnce the pdf pages have been converted to an image format (.png in this example) they can be converted to text using tesseract. The quality of the conversion depends on lots of things, but mostly the quality of the original images. In this example the quality is variable and generally poor, but useful information can still be extracted.\n\ncd ../files/example_files/blog/pdf_extraction\nfor imageFile in *.png\n   do\n   tesseract \\\n   -c tessedit_char_whitelist=\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 :/()-\" \\\n\t\t     $imageFile $imageFile\n   done\n\n\n\nNow that we have freed the information from the confines of the .pdf file we will usualy want to re-assemble the information extracted from each page and clean things up. I'm using R for this, though many of my colleagues prefer python for this sort of thing.\n\ntext_data <- list.files(\"../files/example_files/blog/pdf_extraction\",\n\t\t\tpattern = \"\\\\.txt$\",\n\t\t\tfull.names = TRUE) %>%\n  lapply(FUN = read_lines) %>%\n  lapply(FUN = str_subset, pattern = \"(DATA DESCRIPTION:)|(SOURCE:)|(SIZE:)|(TYPE:)\") %>%\n  lapply(FUN = str_split_fixed, pattern = \": *\", n = 2) %>%\n  lapply(FUN = function(x) {\n    dtmp <- data.frame(as.list(x[, 2]), stringsAsFactors = FALSE)\n    names(dtmp) <- x[, 1]\n    return(dtmp)\n  }) %>%\n  bind_rows()\nhead(text_data)\n\n\n\n                                        DATA DESCRIPTION\n1  Schedule (Sele5:1i2 x-1)yuxgm:glzzezy-    g m:   - nu\n2                            SchedulejSelem-leyugqberm  \n3                                                   <NA>\n4                               City Size Classification\n5 Original Standgrjiqdqstrial Classificatigg- (SIC) pcde\n6        Original Egtabhshcpegt Size Classification u - \n                     SOURCE                        SIZE        TYPE\n1 iEEC/DCC Qqntrol File - m  CHARACTERm): 5 BYTE(S) : 5 Character I\n2                      <NA>    CHARACTERGL: 5 BYTE(5) : : Character\n3     EEC/DCC Contr-ol File     CHARACTERGH 2 BYTE(S) :   Character\n4      EEC/DCC Controi File I CHARACTERS) : 1 BYTE(S) : i Character\n5      EEC/DOC Control File                        <NA>        <NA>\n6      EEC/DOC Control File                        <NA> 0 Character\n\n\n\nIt is cleary that many characters were not recognized correctly. However, there is enough imformation to be useful, especially if we spend a little more effort cleaning things up. The hunspell package in R can be useful if you know the recovered information should be dictionary words.\n\nlibrary(hunspell)\ntext_data[is.na(text_data)] <- \"\"\ntext_data$TYPE <- str_to_lower(text_data$TYPE)\ntext_data$TYPE <- str_replace_all(text_data$TYPE, \"(| ).( |$)\", \"\")\ntype_bad_words <- hunspell(str_c(text_data$TYPE, collapse = \" \"))[[1]]\ntype_replacement_words <- sapply(hunspell_suggest(type_bad_words), function(x) x[[1]])\ntype_bad_words <- str_c(\"(|\\\\W)\",type_bad_words, \"(\\\\W|$)\", sep = \"\")\ntype_replacement_words <- str_c(\"\\\\1\", type_replacement_words, \"\\\\2\")\n\nfor(i in 1:length(type_bad_words)) {\n  text_data$TYPE <- str_replace_all(text_data$TYPE,\n\t\t\t\t    type_bad_words[i],\n\t\t\t\t    type_replacement_words[i])\n}\n\ntext_data$TYPE <- str_replace_all(text_data$TYPE, \" +\", \" \")\ntext_data$TYPE <- str_trim(text_data$TYPE, side = 'both')\n\n\n\nEven after all that there are still some errors, but we've managed to correctly retrieve the type information for the majority of the variables in this dictionary.\n\ncount(text_data, TYPE, sort = TRUE)\n\n\n\n# A tibble: 10 x 2\n             TYPE     n\n            <chr> <int>\n1       character    65\n2   fixed decimal    42\n3                    41\n4      charioteer     1\n5      chm-gage:-     1\n6  fixed decimal)     1\n7  fixed-decimal:     1\n8   fixed deem-ll     1\n9     hexadecimal     1\n10     tee-rater-     1\n\n\n\n\n\nConcluding remarks\n\n\nI covered a lot of ground in this post, from graphical OCR programs to spell checking packages in R. The take-away messages as I seem them are:\n\n\nThe pdftools package is great news for R users who need to work with .pdf files. It makes it easy to extract and manipulate pdf content and metadata no matter what operating system you use, all from within R.\n\nThe tesseract OCR program is very capable, but don't expect miracles. If the original image quality is poor you can expect to spend a lot of time cleaning up the resulting text.\n\n\n\n\n\nFootnotes: \n\n\n1 \nI'm sure these data are available somewhere in more convenient form, but a) I couldn't find them and b) I needed an example pdf with interesting content."
    },
    {
      "title": "Useless but fun R packages",
      "tags": "fun,R",
      "url": "http://people.fas.harvard.edu/~izahn/posts/useless-but-fun-r-packages/",
      "text": "R is useful for many things. But, it is not only useful! There is plenty of fun to be had as well. In celebration of Summer I'm going to take a look at some useless (but fun!) R packages.\n\n\n\n\n\nFortune teller\n\n\nfortunes is probably the best-known \"just for fun\" R package. It is maintained by Achim Zeileis and features contributions from such R luminaries as Peter Dalgaard, Uwe Ligges, Kevin Wright, and many others. The fortunes package been amusing bored statisticians and programmers since 2004. Since that time the fortunes package developers have been selecting amusing quotes from the R-help mailing list and other sources and compiling them for your enjoyment. Let's take a look.\n\n\n## install.packages(\"fortunes\") # if you don't already have it\nlibrary(fortunes)\nfortune()\n\n\n\nWhen in doubt, keep adding slashes until it works.\n   -- Joran Elias (on how to escape a backslash in R)\n      Stackoverflow (March 2015)\n\n\n\nWhen called without arguments, the fortune function will select a random fortune. Calling fortune again will select another random quote:\n\nfortune()\n\n\n\nRAM is cheap and thinking hurts.\n   -- Uwe Ligges (about memory requirements in R)\n      R-help (June 2007)\n\n\n\nIf you want to specify a particular quote you can do so by number or by character search:\n\nfortune(204)\nfortune(\"memory\")\n\n\n\nmemory problems (not me. my pc!)\n   -- Sara Mouro (subject line for an R-help request)\n      R-help (January 2008)\n\nRAM is cheap and thinking hurts.\n   -- Uwe Ligges (about memory requirements in R)\n      R-help (June 2007)\n\n\n\nThat's about it. Well, there are some other options, see ?fortune for the details.\n\n\n\n\n\nCow says what?\n\n\nIf you are a Unix user of a certain age you have no doubt heard of the famous cowsay program. Now R users can join the fun with the cowsay R package by Scott Chamberlain. Like the fortunes package, cowsay exports just one function; say. Let's take a look:\n\n## install.packages(\"cowsay\")\nlibrary(cowsay)\nsay(\"Hello world!\")\n\n\n\n -------------- \nHello world! \n --------------\n    \\\n      \\\n        \\\n            |\\___/|\n          ==) Y (==\n            \\    /\n             )=*=(\n            /     \\\n            |     |\n           /| | | |\\\n           \\| | |_|/\\\n      jgs  //_// ___/\n               \\_)\n\n\n\nCute, but I was led to believe there would be a cow involved! \n\n\nsay(\"Moo may represent an idea, but only the cow knows.\\n --Mason Cooley\",\n    by = \"cow\")\n\n\n\n ----- \nMoo may represent an idea, but only the cow knows.\n --Mason Cooley \n ------ \n    \\   __ \n     \\  (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||\n\n\n\nThere is no option to randomly select an animal, but we can achieve that ourselves easily enough.\n\nsomeone_say_hello <- function() {\n  animal <- sample(names(animals), 1)\n  say(paste(\"Hello, I'm a \", animal, \".\", collapse = \"\"), by = animal)\n  }\nsomeone_say_hello()\n\n\n\n ----- \nHello, I'm a  bigcat . \n ------ \n    \\   \n     \\\n                \\`*-.\n                 )  _`-.\n                .  : `. .\n                : _   '  \\\n                ; *` _.   `*-._\n                `-.-'          `-.\n                  ;       `       `.\n                  :.       .       \\\n                  .\\  .   :   .-'   .\n                  '  `+.;  ;  '      :\n                  :  '  |    ;       ;-.\n                  ; '   : :`-:     _.`* ;\n               .*' /  .*' ; .*`- +'  `*'\n     [bug]     `*-*   `*-*  `*-*'\n\n\n\n\n\nPutting it all together\n\n\nWhen I teach R I emphasize composability. That is, unlike some other statistics packages, R enables you to take the output from one function and pass in on to another. We can take advantage of the excellent composability R provides to do useful things like extract coefficients from a list of model fits and put them into a LaTeX table. Or we can use it to do useless things like making a cow tell us our fortune.\n\n\nsomeone_say_my_fortune <- function(x) {\n  animal <- animal <- sample(names(animals), 1)\n  say(paste(fortune(), collapse = \"\\n\"), by = animal)\n}\nsomeone_say_my_fortune()\n\n\n\n ------------- \nOnly with a very high signal to noise ratio (e.g., high true R2) can \ntorturing data lead to a confession to something other than what the \nanalyst wants to hear.\nFrank Harrell\nNA\nR-help\nApril 2010 \n -------------- \n              \\   \n               \\  \n                \\\n_____________________                              _____________________\n`-._                 \\           |\\__/|           /                 _.-'\n    \\                 \\          |    |          /                 /\n     \\                 `-_______/      \\_______-'                 /\n      |                                                          |\n      |                                                          |\n      |                                                          |\n      /                                                          \\\n     /_____________                                  _____________\\\n                   `----._                    _.----'\n                          `--.            .--'\n                              `-.      .-'\n                                 \\    / :F_P:\n                                  \\  /\n                                   \\/\n\n\n\nBecause fortune gives a random quote each time, and we randomly select animals each time, we will get a surprising new delight every time we call the someone_say_my_fortune function.\n\nsomeone_say_my_fortune()\n\n\n\n ----- \nThere's an informal tradition that those announcements [about R releases]\ncontain at least one mistake, but apparently I forgot this time, so users \nhave to make up their own....\nPeter Dalgaard\nabout an apparent non-bug report in his former R-announce message\nR-help\nDecember 2009 \n ------ \n    \\   \n     \\\n         _\n       _/ }\n      `>' \\\n      `|   \\\n       |   /'-.     .-.\n        \\'     ';`--' .'\n         \\'.    `'-./\n          '.`-..-;`\n            `;-..'\n            _| _|\n            /` /` [nosig]\n\n\n\nIf you want to get really silly about it, you could call that function in your .Rprofile. Or, if you are package author you could add some spice to your warning and error messages by having an ASCII art animal say them. Go forth and have fun!\n\n\n\nIf you know of other useless (but fun!) R packages let me know in the comments."
    },
    {
      "title": "Welcome",
      "tags": "",
      "url": "http://people.fas.harvard.edu/~izahn/pages/about/",
      "text": "Hello, my name is Ista Zahn. Welcome to my blog!. I work as a Data Science Specialist at The Institute for Quantitative Social Science at Harvard University.\n\n\n\nI hold a master's degree in social psychology from the University of Rochester. During graduate school I discovered a love of statistical programming, primarily using the R environment for statistical computing. I am interested in using technology to turn raw data into information useful to human beings. My areas of expertise include data management, graphical displays, regression modeling, and web technologies for statistical computing and teaching.\n\n\n\nHere are some other places in addition to this blog that you can find me on the net.\n\n\n\n github My workshop materials, emacs config etc.\n\n stackoverflow My questions and answers (mostly R-related) on the popular question-and-answer site.\n\n Data Science Services at IQSS My awesome team at the Insitute for Quantitative Social Science at Harvard."
    },
    {
      "title": "Escaping from character encoding hell in R on Windows",
      "tags": "Encoding,Files,R,SHIFT-JIS,UTF-8",
      "url": "http://people.fas.harvard.edu/~izahn/posts/reading-data-with-non-native-encoding-in-r/",
      "text": "Note: the title of this post was inspired by this question on stackoverflow.\n\n\n\nThis section gives the basic facts and recommendations for importing files with arbitrary encoding on Windows. The issues described here by and large to not apply on Mac or Linux; they are specific to running R on Windows.\n\n\n\nIf you are on a deadline and just need to get the job done this section should be all you need. Additional background and discussion is presented in later sections.\n\n\n\nTo read a text file with non ASCII encoding into R you should a) determine the encoding and b) read it in such a way that the information is re-encoded into UTF-8, and c) ignore the bug in the data.frame print method on Windows. Hopefully the encoding is specified in the documentation that accompanied your data. If not, you can guess the encoding using the stri_read_raw and stri_enc_detect functions in the stringi  package. You can ensure that the information is re-encoded to UTF-8 by using the readr package.\n\n\n\n\n\nFor example, I have two versions of a file containing numbers and Japanese characters: japanese_utf8.csv is encoded in UTF-8, and japanese_shiftjis.csv is encoded in SHIFT-JIS. We can read these files as follows on any platform (Windows, Linux, Mac):\n\n\nlibrary(readr)\noptions(stringsAsFactors = FALSE)\nread_csv(\"japanese_utf8.csv\",\n\t locale = locale(encoding = \"UTF-8\"))\nread_csv(\"japanese_shiftjis.csv\",\n\t locale = locale(encoding = \"SHIFT-JIS\"))\n\n\n\n    No.         \u767a\u884c\u65e5 \u671d\u5915\u520a     \u9762\u540d \u30da\u30fc\u30b8\n1 00001 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5    022\n2 00002 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5    018\n3 00003 2015\u5e7409\u670821\u65e5   \u671d\u520a   \uff13\u7dcf\u5408    003\n    No.         \u767a\u884c\u65e5 \u671d\u5915\u520a     \u9762\u540d \u30da\u30fc\u30b8\n1 00001 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5    022\n2 00002 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5    018\n3 00003 2015\u5e7409\u670821\u65e5   \u671d\u520a   \uff13\u7dcf\u5408    003\n\n\n\nOn Windows there is a bug in print.data.frame that causes data.frame's with UTF-8 encoded columns to be displayed incorrectly in non UTF-8 locales. Running the above example on Windows produces this:\n\n\n\n    No.         <U+767A><U+884C><U+65E5> <U+671D><U+5915><U+520A>                 <U+9762><U+540D> <U+30DA><U+30FC><U+30B8>\n1 00001 2015<U+5E74>09<U+6708>25<U+65E5>         <U+9031><U+520A> <U+9031><U+520A><U+671D><U+65E5>                      022\n2 00002 2015<U+5E74>09<U+6708>25<U+65E5>         <U+9031><U+520A> <U+9031><U+520A><U+671D><U+65E5>                      018\n3 00003 2015<U+5E74>09<U+6708>21<U+65E5>         <U+671D><U+520A>                3<U+7DCF><U+5408>                      003\n\n    No.         <U+767A><U+884C><U+65E5> <U+671D><U+5915><U+520A>                 <U+9762><U+540D> <U+30DA><U+30FC><U+30B8>\n1 00001 2015<U+5E74>09<U+6708>25<U+65E5>         <U+9031><U+520A> <U+9031><U+520A><U+671D><U+65E5>                      022\n2 00002 2015<U+5E74>09<U+6708>25<U+65E5>         <U+9031><U+520A> <U+9031><U+520A><U+671D><U+65E5>                      018\n3 00003 2015<U+5E74>09<U+6708>21<U+65E5>         <U+671D><U+520A>                3<U+7DCF><U+5408>                      003\n\n\nwhich looks terrible but does not actually indicate a problem. The information is encoded correctly, but due to a long-standing bug it is displayed incorrectly. You can check to see if the values are correct by converting the data.frame by (ab)using print.listof, e.g.,\n\n\nprint.listof(read_csv(\"japanese_shiftjis.csv\",\n\t\t      locale = locale(encoding = \"SHIFT-JIS\")))\n\n\n\nNo. :\n[1] \"00001\" \"00002\" \"00003\"\n\n\u767a\u884c\u65e5 :\n[1] \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670821\u65e5\"\n\n\u671d\u5915\u520a :\n[1] \"\u9031\u520a\" \"\u9031\u520a\" \"\u671d\u520a\"\n\n\u9762\u540d :\n[1] \"\u9031\u520a\u671d\u65e5\" \"\u9031\u520a\u671d\u65e5\" \"\uff13\u7dcf\u5408\"  \n\n\u30da\u30fc\u30b8 :\n[1] \"022\" \"018\" \"003\"\n\n\n\nTo recap: \n\n\nRegardless of platform (Windows, Mac Linux), use the readr package to read data into R. This will re-encode the contents of the file to UTF-8 for you. \n\nMake sure you specify the encoding using the locale argument as shown in the example above. \n\nIgnore the ugly print.data.frame bug and use print.listof to check that your data was imported correctly.\n\n\n\n\nThose wishing for more details about this issue can read on.\n\n\n\n\nWhat is the problem?\n\n\nThe problem is that the basic R functions for reading and writing data from and to files does no work in any reasonable way on Windows. If you are struggling with this you are not alone! There are numerous questions on stackoverflow, blog posts (e.g., this one by Rolf Fredheim, and another by Huidong Tian), and anguished mailing list posts. Thinking of the person-hours wasted on this issue over the years almost brings a tear to my eye. \n\n\n\nLet's try it, using some simplified data from a project I worked on last year. For illustration I've created two files containing a mix of English letters, numbers, and Japanese characters. I saved one version with UTF-8 encoding, and another with SHIFT-JIS. On Linux we can read both files easily, provided only that we correctly specify the encoding if the file is not already encoded in UTF-8:\n\n\nread.csv(\"japanese_utf8.csv\")\n\n\n\n  No.         \u767a\u884c\u65e5 \u671d\u5915\u520a     \u9762\u540d \u30da\u30fc\u30b8\n1   1 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     22\n2   2 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     18\n3   3 2015\u5e7409\u670821\u65e5   \u671d\u520a   \uff13\u7dcf\u5408      3\n\n\nread.csv(\"japanese_shiftjis.csv\", fileEncoding = \"SHIFT-JIS\")\n\n\n\n  No.         \u767a\u884c\u65e5 \u671d\u5915\u520a     \u9762\u540d \u30da\u30fc\u30b8\n1   1 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     22\n2   2 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     18\n3   3 2015\u5e7409\u670821\u65e5   \u671d\u520a   \uff13\u7dcf\u5408      3\n\n\n\nOn Windows things are much more difficult. Using read.csv with the default options does not work because read.csv assumes that the encoding of the file matches the Windows locale settings:\n\n\nread.csv(\"japanese_utf8.csv\")\n\n\n\n  No.         \u00e7.\u00ba\u00e8.\u0152\u00e6.. \u00e6\u0153.\u00e5..\u00e5.\u0160       \u00e9..\u00e5.. \u00e3\u0192\u0161\u00e3\u0192.\u00e3..\n1   1 2015\u00e5\u00b9\u00b409\u00e6\u0153\u02c625\u00e6\u2014\u00a5    \u00e9\u20ac\u00b1\u00e5\u02c6\u0160 \u00e9\u20ac\u00b1\u00e5\u02c6\u0160\u00e6\u0153\u009d\u00e6\u2014\u00a5        22\n2   2 2015\u00e5\u00b9\u00b409\u00e6\u0153\u02c625\u00e6\u2014\u00a5    \u00e9\u20ac\u00b1\u00e5\u02c6\u0160 \u00e9\u20ac\u00b1\u00e5\u02c6\u0160\u00e6\u0153\u009d\u00e6\u2014\u00a5        18\n3   3 2015\u00e5\u00b9\u00b409\u00e6\u0153\u02c621\u00e6\u2014\u00a5    \u00e6\u0153\u009d\u00e5\u02c6\u0160    \u00ef\u00bc\u201c\u00e7\u00b7\u008f\u00e5\u0090\u02c6         3\n\n\n\nTrying to tell R that the file is encoded in UTF-8 not a general solution because read.csv will then try to re-encode from UTF-8 to the native encoding, which may or may not work depending on the contents of the file. On my system trying to read a UTF-8 encoded file containing Japanese characters with the fileEncoding falls flat on its face:\n\nread.csv(\"japanese_utf8.csv\", fileEncoding = \"UTF-8\")\n\n\n\n[1] No. X  \n<0 rows> (or 0-length row.names)\nWarning messages:\n1: In read.table(file = file, header = header, sep = sep, quote = quote,  :\n  invalid input found on input connection 'japanese_utf8.csv'\n2: In read.table(file = file, header = header, sep = sep, quote = quote,  :\n  incomplete final line found by readTableHeader on 'japanese_utf8.csv'\n\n\n\n\nFinally, we might try the encoding argument rather than fileEncoding. This simply marks the strings with the specified encoding:\n\nread.csv(\"japanese_utf8.csv\", encoding = \"UTF-8\")\n\n\n\nread.csv(\"japanese_utf8.csv\", encoding = \"UTF-8\")\n  No.        X.U.767A..U.884C..U.65E5. X.U.671D..U.5915..U.520A.                X.U.9762..U.540D. X.U.30DA..U.30FC..U.30B8.\n1   1 2015<U+5E74>09<U+6708>25<U+65E5>          <U+9031><U+520A> <U+9031><U+520A><U+671D><U+65E5>                        22\n2   2 2015<U+5E74>09<U+6708>25<U+65E5>          <U+9031><U+520A> <U+9031><U+520A><U+671D><U+65E5>                        18\n3   3 2015<U+5E74>09<U+6708>21<U+65E5>          <U+671D><U+520A>                3<U+7DCF><U+5408>                         3\n\n\nThis kind of works, though you wouldn't know it from the output. As mentioned above, there is a bug in the print.data.frame function that prevents UTF-8 encoded text from displaying correctly. We can use another print method to see that the column values have been read in correctly:\n\nprint.listof(read.csv(\"japanese_utf8.csv\", encoding = \"UTF-8\"))\n\n\n\nNo. :\n[1] 1 2 3\n\nX.U.767A..U.884C..U.65E5. :\n[1] \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670821\u65e5\"\n\nX.U.671D..U.5915..U.520A. :\n[1] \"\u9031\u520a\" \"\u9031\u520a\" \"\u671d\u520a\"\n\nX.U.9762..U.540D. :\n[1] \"\u9031\u520a\u671d\u65e5\" \"\u9031\u520a\u671d\u65e5\" \"\uff13\u7dcf\u5408\"  \n\nX.U.30DA..U.30FC..U.30B8. :\n[1] 22 18  3\n\n\n\nUnfortunately there are two problems with this: first, the names of the columns have not been correctly encoded, and second, this will only work if your input data is in UTF-8 in the first place. Trying to apply this strategy to our SHIFT-JIS encoded file will not work at all because we cannot mark strings with arbitrary encoding, only with UTF-81. Trying to mark the string as SHIFT-JIS will silently fail:\n\nprint.listof(read.csv(\"japanese_shiftjis.csv\", encoding = \"SHIFT-JIS\"))\n\n\n\nNo. :\n[1] 1 2 3\n\nX...s.\u00fa :\n[1] \"2015\u201dN09\u0152\u017d25\u201c\u00fa\" \"2015\u201dN09\u0152\u017d25\u201c\u00fa\" \"2015\u201dN09\u0152\u017d21\u201c\u00fa\"\n\nX....\u0160. :\n[1] \"\u008fT\u0160\u00a7\" \"\u008fT\u0160\u00a7\" \"\u2019\u00a9\u0160\u00a7\"\n\nX.\u00ca.. :\n[1] \"\u008fT\u0160\u00a7\u2019\u00a9\u201c\u00fa\" \"\u008fT\u0160\u00a7\u2019\u00a9\u201c\u00fa\" \"\u201aR\u2018\u008d\u008d\u2021\"  \n\n\u0192y..\u0192W :\n[1] 22 18  3\n\n\n\nOuch! Why is this so hard? Can we make it suck less?\n\n\n\n\n\nEncoding in R\n\n\nBasically R gives you two ways of handling character encoding. You can use the default encoding of your OS, or you can use UTF-81. On OS X and Linux these options are often the same, since the default OS encoding is usually UTF-8; this is a great advantage because just about everything can be represented in UTF-8. On Windows there is no such luck. On my Windows 7 machine the default is \"Windows code page 1252\"; many characters (such as Japanese) cannot be represented in code page 1252. If I want to work with Japanese text in R on Windows I have two options; change my locale to Japanese, or I can convert strings to UTF-8 and mark them as such.\n\n\n\nIn some ways just changing your locale to one that can accommodate the data you are working with is the simplest approach. Again, on Mac and Linux the locale usually specifies UTF-8 encoding, so no changes are needed; things should just work as you would expect them to. On windows we can change the locale to match the data we are working with using the Sys.setlocale function. This sometimes works well; for example, we can read our UTF-8 and SHIFT-JIS encoded data on Windows as follows:\n\n\nsetlocale(\"LC_ALL\", \"English_United States.932\")\nread.csv(\"japanese_shiftjis.csv\")\nread.csv(\"japanese_utf8.csv\", fileEncoding = \"UTF-8\")\n\n\n\n[1] \"LC_COLLATE=English_United States.932;LC_CTYPE=English_United States.932;LC_MONETARY=English_United States.932;LC_NUMERIC=C;LC_TIME=English_United States.932\"\n\n  No.         \u767a\u884c\u65e5 \u671d\u5915\u520a     \u9762\u540d \u30da\u30fc\u30b8\n1   1 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     22\n2   2 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     18\n3   3 2015\u5e7409\u670821\u65e5   \u671d\u520a   \uff13\u7dcf\u5408      3\n\n  No.         \u767a\u884c\u65e5 \u671d\u5915\u520a     \u9762\u540d \u30da\u30fc\u30b8\n1   1 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     22\n2   2 2015\u5e7409\u670825\u65e5   \u9031\u520a \u9031\u520a\u671d\u65e5     18\n3   3 2015\u5e7409\u670821\u65e5   \u671d\u520a   \uff13\u7dcf\u5408      3\n\n\n\nThis works fine until we want to read some other kind of text in the same R session, and then we are right back to the same old problem. Another issue with this method is that it does not work in Rstudio unless the locale is set on startup; you cannot change the locale of a running session in Rstudio2.\n\n\n\nBecause the Sys.setlocale method only works for a subset of languages in any given session, our best bet is to read store everything in UTF-8 (and make sure it is marked as such). It is not convenient to do this using the read.table family of functions in R, but it is possible with some care:\n\nx <- read.csv(\"japanese_shiftjis.csv\", \n\t      encoding = \"UTF-8\", \n\t      check.names = FALSE # otherwise R will mangle the names\n\t      )\ncharcols <- !sapply(x, is.numeric)\nx[charcols] <- lapply(x[charcols], iconv, from = \"SHIFT-JIS\", to = \"UTF-8\")\nnames(x) <- iconv(names(x), from = \"SHIFT-JIS\", to = \"UTF-8\")\nprint.listof(x)\n\n\n\nNo. :\n[1] 1 2 3\n\n\u767a\u884c\u65e5 :\n[1] \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670821\u65e5\"\n\n\u671d\u5915\u520a :\n[1] \"\u9031\u520a\" \"\u9031\u520a\" \"\u671d\u520a\"\n\n\u9762\u540d :\n[1] \"\u9031\u520a\u671d\u65e5\" \"\u9031\u520a\u671d\u65e5\" \"\uff13\u7dcf\u5408\"  \n\n\u30da\u30fc\u30b8 :\n[1] 22 18  3\n\n\nOK it works, but honestly that too much work for something as simple as reading a .csv file into R. As suggested at the beginning of this post, a better strategy is to use the readr package because it will do the conversion to UTF-8 for you:\n\n\nprint.listof(read_csv(\"arabic_utf-8.csv\"), locale = locale(encoding = \"UTF-8\"))\nprint.listof(read_csv(\"japanese_utf8.csv\"), locale = locale(encoding = \"UTF-8\"))\nprint.listof(read_csv(\"japanese_shiftjis.csv\"), locale = locale(encoding = \"SHIFT-JIS\"))\n\n\n\nX5 :\n[1] \"1895-01-02\" \"1895-01-07\" \"1895-01-16\"\nX8 :\n[1] \"\u0627\u0635\u0644\u0649\" \"\u0627\u0635\u0644\u0649\" \"\u0627\u0635\u0644\u0649\"\nX12 :\n[1] \"\u0648\u0642\u0627\u0626\u0639\" \"\u0648\u0642\u0627\u0626\u0639\" \"\u0648\u0642\u0627\u0626\u0639\"\n\nNo. :\n[1] \"00001\" \"00002\" \"00003\"\n\u767a\u884c\u65e5 :\n[1] \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670821\u65e5\"\n\u671d\u5915\u520a :\n[1] \"\u9031\u520a\" \"\u9031\u520a\" \"\u671d\u520a\"\n\u9762\u540d :\n[1] \"\u9031\u520a\u671d\u65e5\" \"\u9031\u520a\u671d\u65e5\" \"\uff13\u7dcf\u5408\"  \n\u30da\u30fc\u30b8 :\n[1] \"022\" \"018\" \"003\"\n\n\nNo. :\n[1] \"00001\" \"00002\" \"00003\"\n\u767a\u884c\u65e5 :\n[1] \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670825\u65e5\" \"2015\u5e7409\u670821\u65e5\"\n\u671d\u5915\u520a :\n[1] \"\u9031\u520a\" \"\u9031\u520a\" \"\u671d\u520a\"\n\u9762\u540d :\n[1] \"\u9031\u520a\u671d\u65e5\" \"\u9031\u520a\u671d\u65e5\" \"\uff13\u7dcf\u5408\"  \n\u30da\u30fc\u30b8 :\n[1] \"022\" \"018\" \"003\"\n\n\n\n\n\nFiles\n\n\nHere are the example data files and code and needed to run the examples in this post.\n\n\n\n\nFootnotes: \n\n\n1 \nWe can also mark strings as encoded in latin1, but that is not useful if you take my advice and store everything in UTF-8.\n\n\n2 \nSee https://support.rstudio.com/hc/en-us/articles/200532197-Character-Encoding"
    }
  ]
}